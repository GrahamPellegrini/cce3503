{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Feature analysis\n",
    "\n",
    "Check that an older version of numpy is being used by the system since numpy versions past 2.00 cause mlxtend due to some NPN metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:38.068894Z",
     "iopub.status.busy": "2024-11-13T12:12:38.067196Z",
     "iopub.status.idle": "2024-11-13T12:12:39.322505Z",
     "shell.execute_reply": "2024-11-13T12:12:39.321205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the dataset from https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized saved as a csv file \"CommViolPredUnnormalizedData.csv\". The dataset has 147 features of which; 125 predictive, 4 non-predictive, 18 potential goal features. The goal feature to be taken is the assultPerPop feature which describes the number of assaults per 100,000 population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:39.373958Z",
     "iopub.status.busy": "2024-11-13T12:12:39.373462Z",
     "iopub.status.idle": "2024-11-13T12:12:44.043627Z",
     "shell.execute_reply": "2024-11-13T12:12:44.043102Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_864101/3920488415.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/arrays/arrow/array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3/dist-packages/numexpr/__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_864101/3920488415.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/arrays/arrow/array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/core/nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"/opt/users/gpel0001/.local/lib/python3.10/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/usr/lib/python3/dist-packages/bottleneck/__init__.py\", line 2, in <module>\n",
      "    from .reduce import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CommViolPredUnnormalizedData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load Dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCommViolPredUnnormalizedData.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get the shape of the dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Raw Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CommViolPredUnnormalizedData.csv'"
     ]
    }
   ],
   "source": [
    "# Importing pandas for loading the dataset and data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Load Dataset\n",
    "dataset = pd.read_csv(\"assignment-1/CommViolPredUnnormalizedData.csv\", header=0)\n",
    "\n",
    "# Get the shape of the dataset\n",
    "print(\"Dataset Raw Shape:\", dataset.shape)\n",
    "\n",
    "# Print the headers to inspect them\n",
    "print(\"Dataset Headers:\", dataset.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data cleaning, missing data and normalization\n",
    "### Drop Non-Predictive and Goal Columns\n",
    "Remove the columns that are not predictive or are potential goals that arent the target column. The working dataset should only contain predictive features and the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:44.045375Z",
     "iopub.status.busy": "2024-11-13T12:12:44.045185Z",
     "iopub.status.idle": "2024-11-13T12:12:44.064750Z",
     "shell.execute_reply": "2024-11-13T12:12:44.064303Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m headers_to_drop \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m non_predicitive_headers \u001b[38;5;241m+\u001b[39m goal_headers\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Check if the headers exist in the dataset if not dont add them to the drop list as they will throw an error\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m headers_to_drop \u001b[38;5;241m=\u001b[39m [header \u001b[38;5;28;01mfor\u001b[39;00m header \u001b[38;5;129;01min\u001b[39;00m headers_to_drop \u001b[38;5;28;01mif\u001b[39;00m header \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Drop the columns\u001b[39;00m\n\u001b[1;32m     14\u001b[0m dataset\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mheaders_to_drop, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m headers_to_drop \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m non_predicitive_headers \u001b[38;5;241m+\u001b[39m goal_headers\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Check if the headers exist in the dataset if not dont add them to the drop list as they will throw an error\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m headers_to_drop \u001b[38;5;241m=\u001b[39m [header \u001b[38;5;28;01mfor\u001b[39;00m header \u001b[38;5;129;01min\u001b[39;00m headers_to_drop \u001b[38;5;28;01mif\u001b[39;00m header \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Drop the columns\u001b[39;00m\n\u001b[1;32m     14\u001b[0m dataset\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mheaders_to_drop, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a list of non-predictive headers\n",
    "non_predicitive_headers = ['communityname', 'countyCode', 'communityCode', 'fold']\n",
    "\n",
    "# Define a list of potential goal headers\n",
    "goal_headers = ['murders', 'murdPerPop', 'rapes', 'rapesPerPop', 'robberies', 'robbbPerPop', 'assaults', 'burglaries', 'burglPerPop', 'larcenies', 'larcPerPop', 'autoTheft', 'autoTheftPerPop', 'arsons', 'arsonsPerPop', 'ViolentCrimesPerPop', 'nonViolPerPop']\n",
    "\n",
    "# Combine both lists of headers and also drop the index column from the csv format\n",
    "headers_to_drop = ['Unnamed: 0'] + non_predicitive_headers + goal_headers\n",
    "\n",
    "# Check if the headers exist in the dataset if not dont add them to the drop list as they will throw an error\n",
    "headers_to_drop = [header for header in headers_to_drop if header in dataset.columns]\n",
    "\n",
    "# Drop the columns\n",
    "dataset.drop(columns=headers_to_drop, inplace=True)\n",
    "\n",
    "# Print the shape of the dataset after dropping columns\n",
    "print(\"Dataset Shape after dropping columns:\", dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handeling of Non-Numeric Features\n",
    "\n",
    "The non-numeric features can not be processed noramally through mathematical operations that we require, such as normalization, so we need to convert them to a numeric format.\n",
    "\n",
    "For the state we are able to use a dictionary to map each unique state to a unique numeric value without influencing the prediction since it is a category identifier.\n",
    "\n",
    "For the other non-numeric features we can use one-hot encoding to convert them to a numeric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:44.066437Z",
     "iopub.status.busy": "2024-11-13T12:12:44.066288Z",
     "iopub.status.idle": "2024-11-13T12:12:44.079457Z",
     "shell.execute_reply": "2024-11-13T12:12:44.078952Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Replace missing values indicated by '?' with np.nan values\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Note this is done here to avoid errors when converting to numeric but will also be needed for imputation\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Print the non-numeric features\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-Numeric Features:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace missing values indicated by '?' with np.nan values\n",
    "dataset = dataset.replace('?', np.nan)\n",
    "# Note this is done here to avoid errors when converting to numeric but will also be needed for imputation\n",
    "\n",
    "# Print the non-numeric features\n",
    "print(\"Non-Numeric Features:\", dataset.select_dtypes(exclude='number').columns.tolist())\n",
    "\n",
    "# Get the unique instances of the state column\n",
    "states = dataset['state'].unique()\n",
    "\n",
    "# Create a dictionary for each unique state and its corresponding numeric value\n",
    "state_dict = {state: i for i, state in enumerate(states)}\n",
    "\n",
    "# Map the state column to the dictionary\n",
    "dataset['state'] = dataset['state'].map(state_dict)\n",
    "\n",
    "# Convert the dataset to a numeric format\n",
    "dataset = dataset.apply(pd.to_numeric)\n",
    "\n",
    "# Check if all features are numeric\n",
    "print(\"All Features are Numeric:\", dataset.select_dtypes(exclude='number').empty)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handeling Missing Values \n",
    "\n",
    "1. Remove rows/columns with missing values. It is a simple implementation but can lead to a lot of data loss if there are many missing data occurances.\n",
    "\n",
    "2. Foward or Backward Filling is the method of using previous or next column values (respectively). It is typicaly useful for sequential data.\n",
    "\n",
    "3. Imputation is the filling of data using statistical methods such as the mean, median or mode. Simple to implement without loss of data. However, may cause heavy biases if the data is not randomly missing.\n",
    "\n",
    "4. Specified Algorthimns in the machine learning algorithm can handle the case of missing values. Being specific to the algorithmn can come as a advantage and a constraint.\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "Dropping the rows with missing values resulted in too many rows being removed. Causing the dataset to be reduced from 2215 rows to 337 rows. This is a significant loss of data and will result in a less accurate model.\n",
    "\n",
    "\n",
    "    Dataset Shape before dropping rows with missing values: (2215, 126)\n",
    "    \n",
    "    Dataset Shape after dropping rows with missing values: (337, 126)\n",
    "\n",
    "Given the large proportion of missing values and the presence of consecutive gaps, forward or backward filling would not effectively address the gaps and could introduce biases in the dataset.\n",
    "    \n",
    "**Chosen Method** \n",
    "\n",
    "A hybrid approach of dropping columns that are heavy with missing values and imputing the remaining missing values was chosen to handle the missing values in the dataset. This approach seemed fit as it eliminated the bias that would have been introduced by imputing the heavy missing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:44.081081Z",
     "iopub.status.busy": "2024-11-13T12:12:44.080839Z",
     "iopub.status.idle": "2024-11-13T12:12:53.388287Z",
     "shell.execute_reply": "2024-11-13T12:12:53.387473Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print the number of missing values in the dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of missing values in the dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Print the shape before dropping missing heavy columns\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Shape before dropping missing heavy columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Import the SimpleImputer class\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Print the number of missing values in the dataset\n",
    "print(\"Number of missing values in the dataset:\", dataset.isnull().sum().sum())\n",
    "\n",
    "# Print the shape before dropping missing heavy columns\n",
    "print(\"Dataset Shape before dropping missing heavy columns:\", dataset.shape)\n",
    "\n",
    "# Drop columns with more than 50% missing values\n",
    "dataset.dropna(thresh=0.5*dataset.shape[0], axis=1, inplace=True)\n",
    "\n",
    "# Print the shape after dropping missing heavy columns\n",
    "print(\"Dataset Shape after dropping missing heavy columns:\", dataset.shape)\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(\"Number of missing values after dropping missing heavy columns:\", dataset.isnull().sum().sum())\n",
    "\n",
    "# Initialize the SimpleImputer with mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer using fit_transform on the dataset for the remaining missing values\n",
    "dataset = pd.DataFrame(imputer.fit_transform(dataset), columns=dataset.columns)\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(\"Number of missing values after imputation: \", dataset.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Dataset\n",
    "\n",
    "Normalization transforms the data to a specific range or distribution, making it easier for machine learning algorithms to process. It ensures that each feature contributes equally to the model, preventing features with larger ranges from dominating the learning process. This is crutial in algorithmns such as the gradient-decent.\n",
    "\n",
    "1. Min-Max Scaling transforms the data to a range [0,1] with a simple formulea. This preserves the relation between data and points on grapichal representations. However, it is sensitive to outlires that may come outside the original min/max.\n",
    "\n",
    "2. Standardization transforms the data to have a mean of 0 and a standard deviation of 1. It is effective for algorithms that assume normally distributed data but is also sensitive to outliers.\n",
    "\n",
    "3. Robust Scaling uses the median and interquartile range for scaling. This fixes the problem of outliers but makes it suffer from data that is not symettrically distributed.\n",
    "\n",
    "4. Max Abs Scaling, scales each feature by its maximum absolute value. Particularly useful for data that is already centered at zero but does not handle outliers well either.\n",
    "\n",
    "*** Evalutation ***\n",
    "\n",
    "When testing using both Min-Max scaling and Standard Scalar. I noted a significant difference in the compared MSE values. This was due to the different scaling ranges of the two methods. Min-Max scales to a 0-1 range whilst StandardScaler scales to a mean of 0 and a standard deviation of 1. So although the Min-Max seemed to have better MSE values, it was not magnitudes better but rather a scaling difference.\n",
    "\n",
    "*** Chosen Method ***\n",
    "\n",
    "Standard Scalar normalization was chosen due to the varience of scaling between the features and its function of having mean 0 and variance 1, seemed fitting for the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:53.391269Z",
     "iopub.status.busy": "2024-11-13T12:12:53.390685Z",
     "iopub.status.idle": "2024-11-13T12:12:53.407601Z",
     "shell.execute_reply": "2024-11-13T12:12:53.407132Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Fit and transform the dataset\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m norm_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdataset\u001b[49m), columns\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Print the head of the dataset\u001b[39;00m\n\u001b[1;32m     11\u001b[0m norm_dataset\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the StandardScaler class\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the dataset\n",
    "norm_dataset = pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)\n",
    "\n",
    "# Print the head of the dataset\n",
    "norm_dataset.head()\n",
    "\n",
    "target = norm_dataset['assaultPerPop']\n",
    "features = norm_dataset.drop(columns=['assaultPerPop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter methods\n",
    "\n",
    "### Colour Coded Correlation Matrix\n",
    "\n",
    "Using pandas defined .corr() function to calculate the correlation matrix of the numeric columns in the dataset. We are easily able to see the correlation between the features in the dataset. These are then plotted using matplotlib and seaborn to create a heatmap of the correlation matrix. This allows us to easily see the correlation between the features in the dataset.\n",
    "\n",
    "We note the diagonal line of correlation of 1.0 as expected due to the feature being correlated with itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:53.409256Z",
     "iopub.status.busy": "2024-11-13T12:12:53.409088Z",
     "iopub.status.idle": "2024-11-13T12:12:59.479471Z",
     "shell.execute_reply": "2024-11-13T12:12:59.478999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/users/gpel0001/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'norm_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m target_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massaultPerPop\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define the correlation matrix\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mnorm_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Plot the heatmap\u001b[39;00m\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m20\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'norm_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the matplotlib and seaborn libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the goal feature as the correlation target\n",
    "target_id = 'assaultPerPop'\n",
    "\n",
    "# Define the correlation matrix\n",
    "correlation_matrix = norm_dataset.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(25, 20))\n",
    "# Sns heatmap with correlation matrix, with cmap as coolwarm, so red is positive, blue is negative\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', cbar=True)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Selecting a Correlation Threshold\n",
    "\n",
    "When we calculated the correlation matrix, we found the index of mutual information between all the features. Now we will use this index to select the features that have are highly correlated with the target feature.\n",
    "\n",
    "1. Visually inspect the correlation matrix and select a threshold that makes sense based off the heatmap and the correlation values.\n",
    "\n",
    "2. Knowing the area/domain of the data and the features, select a threshold that makes sense. So if police where to look at the dataset they could identify the features that are highly correlated with each other and select a threshold that makes sense.\n",
    "\n",
    "3. Use common statistical thresholds, so between -1 and 1, a correlation of 0.7 is considered high, 0.5 is moderate and 0.3 is low. So you can select a threshold of 0.5 for example. These values have statistical significance and are commonly.\n",
    "\n",
    "4. Itersatively select a threshold and evaluate the model's performance. So you can start with a threshold of 0.5 and evaluate the model's performance, then increase or decrease the threshold and evaluate the model's performance again. This is a trial and error method but it is effective.\n",
    "\n",
    "A moderate correlation threshold of 0.5 is used to select the features. This is mainly due to the fact that higher could not be used as there wouldn't be any dtypes left to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.481297Z",
     "iopub.status.busy": "2024-11-13T12:12:59.481103Z",
     "iopub.status.idle": "2024-11-13T12:12:59.494209Z",
     "shell.execute_reply": "2024-11-13T12:12:59.493759Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correlation_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m correlation_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Select the features based on the correlation absolute threshold value, that is taking both negative and positive scales\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m \u001b[43mcorrelation_matrix\u001b[49m\u001b[38;5;241m.\u001b[39mindex[correlation_matrix[target_id]\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m>\u001b[39m correlation_threshold]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Remove the target feature from the selected features\u001b[39;00m\n\u001b[1;32m      8\u001b[0m selected_features\u001b[38;5;241m.\u001b[39mremove(target_id)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'correlation_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Select features based on a correlation threshold\n",
    "correlation_threshold = 0.5\n",
    "\n",
    "# Select the features based on the correlation absolute threshold value, that is taking both negative and positive scales\n",
    "selected_features = correlation_matrix.index[correlation_matrix[target_id].abs() > correlation_threshold].tolist()\n",
    "\n",
    "# Remove the target feature from the selected features\n",
    "selected_features.remove(target_id)  \n",
    "\n",
    "# Print the amount of selected features\n",
    "print(\"Number of Selected Features from Correlation Threshold:\", len(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "The dataset must now be split into training and testing sets. The training set is used to train the model, while the testing set is used to evaluate the model's performance. It is vital that there is no data leakage between the sets or else the model performance will be overestimated.\n",
    "\n",
    "The split ratio is chosen arbitrarily and can be to the size of training or testing wanted. However, when building models, it is common to use an 80/20 or 70/30 split. In this case, we will use an 80/20 split. It is important to train on more data than testing to ensure the model is robust.\n",
    "\n",
    "After splitting the normalized dataset. We can get the selected sets from the features chosen based on the correlation threshold. The selected features training set is then fitted to the model, and the testing set is used to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.496118Z",
     "iopub.status.busy": "2024-11-13T12:12:59.495680Z",
     "iopub.status.idle": "2024-11-13T12:12:59.509979Z",
     "shell.execute_reply": "2024-11-13T12:12:59.509538Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mfeatures\u001b[49m, target, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get the selected features for training and testing\u001b[39;00m\n\u001b[1;32m      9\u001b[0m X_train_flt \u001b[38;5;241m=\u001b[39m X_train[selected_features]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get the selected features for training and testing\n",
    "X_train_flt = X_train[selected_features]\n",
    "X_test_flt = X_test[selected_features]\n",
    "\n",
    "# Print the size of the training and testing sets\n",
    "print(\"Training Set Size:\", X_train.shape)\n",
    "print(\"Testing Set Size:\", X_test.shape)\n",
    "print(\"-------------------------\")\n",
    "print(\"After Feature Selection\")\n",
    "print(\"Training Set Size:\", X_train_flt.shape)\n",
    "print(\"Testing Set Size:\", X_test_flt.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Network for Prediction\n",
    "\n",
    "The MLPRegressor class is a predictive model used to predict continuous values. Unlike MLPClassifier, which is used for classification tasks. We initialize the regressor with base parameters as the filter method should not take too many computational resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.511671Z",
     "iopub.status.busy": "2024-11-13T12:12:59.511520Z",
     "iopub.status.idle": "2024-11-13T12:12:59.549269Z",
     "shell.execute_reply": "2024-11-13T12:12:59.548815Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_flt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLPRegressor(hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m,), max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Fit the model on the training data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m mlp_flt \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_flt\u001b[49m, y_train)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Predict the target feature on the test data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m y_pred_flt \u001b[38;5;241m=\u001b[39m mlp_flt\u001b[38;5;241m.\u001b[39mpredict(X_test_flt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_flt' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the MLPRegressor class\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Initialize the MLPRegressor \n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "mlp_flt = mlp.fit(X_train_flt, y_train)\n",
    "\n",
    "# Predict the target feature on the test data\n",
    "y_pred_flt = mlp_flt.predict(X_test_flt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "The sklearn library provides a mean_squared_error function to compute the MSE between the actual and predicted values.\n",
    "\n",
    "The lower the MSE the better the model is performing. Keeping in mind the scalling of normalized data due to the StandardScaler, the MSE value should be close to 1.0 or less for a good model.\n",
    "\n",
    "In this case, the MSE value is 0.43337 which is a very good value for the model and we may see the other models find it hard to improve this value significantly further. \n",
    "\n",
    "A plot of the actual vs predicted values is also displayed. This can give us a visual representation of how close to perfect correlation the model is operating. The closer the points are to the red line, the better the model is performing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.551058Z",
     "iopub.status.busy": "2024-11-13T12:12:59.550895Z",
     "iopub.status.idle": "2024-11-13T12:12:59.563831Z",
     "shell.execute_reply": "2024-11-13T12:12:59.563285Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compute the Mean Squared Error (MSE) for the filter method\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m mse_flt \u001b[38;5;241m=\u001b[39m mean_squared_error(\u001b[43my_test\u001b[49m, y_pred_flt)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Print the Mean Squared Error (MSE) for the filter method\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Squared Error (MSE) for the filter method:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mse_flt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the mean_squared_error function\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) for the filter method\n",
    "mse_flt = mean_squared_error(y_test, y_pred_flt)\n",
    "\n",
    "# Print the Mean Squared Error (MSE) for the filter method\n",
    "print(\"Mean Squared Error (MSE) for the filter method:\", mse_flt)\n",
    "\n",
    "# Plot the predicted vs actual values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, y_pred_flt, color='blue')\n",
    "# Add a line for perfect correlation\n",
    "plt.plot(y_test, y_test, color='red')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Filter Method (Actual vs Predicted Values)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Wrapper methods\n",
    "### Sequential Forward Selection (SFS)\n",
    "\n",
    "The SFS is a method of feature selection in which we start with an empty set of features and add features one by one. At each iteration, we analyze the performance of the model with the added feature and select the best feature based on a predefined criterion. The process continues until we tested all the features or reached a predefined number of features. The best number and combination of features are then selected based on the predefined criterion.\n",
    "\n",
    "We redifine the MLPRegressor for SFS and SBS with early stopping parameters to avoid contining trying to fit the model if it is not improving. Furthermore, we set n_iter_no_change to avoid epochs with no changes.\n",
    "\n",
    "We then define the sfs method using the SequentialFeatureSelector class from the mlxtend library. For SFS we set foward=True, which means we are using the forward selection method and we make sure that the k_features is set to 40, which is the maximum number of features we want to select. We are also evaluating the model using mse so we set scoring='neg_mean_squared_error' and cv=0, which means we are not using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.565525Z",
     "iopub.status.busy": "2024-11-13T12:12:59.565374Z",
     "iopub.status.idle": "2024-11-13T12:12:59.639369Z",
     "shell.execute_reply": "2024-11-13T12:12:59.638848Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m sfs \u001b[38;5;241m=\u001b[39m SF(  mlp,\n\u001b[1;32m      9\u001b[0m             k_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m     10\u001b[0m             forward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m             floating\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m             scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m             cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Select the features using the Sequential Forward Selection\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m sfs \u001b[38;5;241m=\u001b[39m sfs\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Get the selected features after SFS\u001b[39;00m\n\u001b[1;32m     18\u001b[0m sfs_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sfs\u001b[38;5;241m.\u001b[39mk_feature_names_)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the SequentialFeatureSelector class\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SF\n",
    "\n",
    "# Redefine the MLPRegressor with early stopping parameters to avoid contining trying to fit the model if it is not improving and n_iter_no_change to avoid epochs with no changes.\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(50,), max_iter=500, early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# Define the Sequential Forward Selection (SFS)\n",
    "sfs = SF(  mlp,\n",
    "            k_features=40,\n",
    "            forward=True,\n",
    "            floating=False,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=0)\n",
    "\n",
    "# Select the features using the Sequential Forward Selection\n",
    "sfs = sfs.fit(X_train, y_train)\n",
    "# Get the selected features after SFS\n",
    "sfs_features = list(sfs.k_feature_names_)\n",
    "\n",
    "# Print the size of the selected features\n",
    "print(\"Number of Selected Features from SFS:\", len(sfs_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Backward Selection (SBS)\n",
    "\n",
    "The SBS is similar to the SFS but in reverse, this is why when defining the SBS object, the forward parameter is set to False. The algorithm will start with all the features and remove them one by one until the desired number of features is reached or the performance metric stops improving. The SBS algorithm is useful when the dataset has a large number of features and you want to reduce the number of features to improve the model's performance.\n",
    "\n",
    "However, can suffer from overfitting if the number of features is too high, and the model will not generalize well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.641277Z",
     "iopub.status.busy": "2024-11-13T12:12:59.640999Z",
     "iopub.status.idle": "2024-11-13T12:12:59.654829Z",
     "shell.execute_reply": "2024-11-13T12:12:59.654361Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m sbs \u001b[38;5;241m=\u001b[39m SF(mlp,\n\u001b[1;32m      3\u001b[0m             k_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m      4\u001b[0m             forward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m             floating\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m             scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m             cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Select the features using the Sequential Backward Selection\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m sbs \u001b[38;5;241m=\u001b[39m sbs\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Get the selected features after SBS\u001b[39;00m\n\u001b[1;32m     13\u001b[0m sbs_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sbs\u001b[38;5;241m.\u001b[39mk_feature_names_)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the Sequential Backward Selection (SBS) object\n",
    "sbs = SF(mlp,\n",
    "            k_features=40,\n",
    "            forward=False,\n",
    "            floating=False,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=0)\n",
    "\n",
    "# Select the features using the Sequential Backward Selection\n",
    "sbs = sbs.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features after SBS\n",
    "sbs_features = list(sbs.k_feature_names_)\n",
    "\n",
    "# Print the size of the selected features\n",
    "print(\"Number of Selected Features from SBS:\", len(sbs_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion on SFS and SBS\n",
    "\n",
    "\n",
    "**Simmilarities:**\n",
    "- Both SFS and SBS are easy to implement as they involve iterative feature selection with additional or removal of features.\n",
    "- They are both wrapper methods that use a predictive model to evaluate the feature subsets.\n",
    "- They both use greedy search strategies to select features which may lead to suboptimal solutions. This is they are unable to see the bigger picture and may not be optimal.\n",
    "\n",
    "**Strengths of SFS:**\n",
    "- SFS is computationally less expensive compared to exhaustive search methods.\n",
    "- It is useful when the number of features is large, as it incrementally adds features that improve the model.\n",
    "\n",
    "**Limitations of SFS:**\n",
    "- SFS can get stuck in local optima, as it does not consider removing features once added.\n",
    "- It may include redundant features that do not contribute significantly to the model's performance.\n",
    "\n",
    "**Strengths of SBS:**\n",
    "- SBS can remove irrelevant or redundant features, potentially leading to a more compact and interpretable model.\n",
    "- It can be useful when the initial feature set is large and contains many irrelevant features.\n",
    "\n",
    "**Limitations of SBS:**\n",
    "- SBS is computationally more expensive than SFS, as it starts with all features and removes them one by one.\n",
    "- It may remove features that are individually weak but collectively strong, leading to suboptimal feature subsets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Networks\n",
    "\n",
    "After defining the objects, we now select the features using the SFS and SBS gathered selected features. We then train the neural network using the selected features and predict the target feature on the test data. Finally, we compute the Mean Squared Error (MSE) for both the SFS and SBS selected features. Similarly, we plot the predicted vs actual values for the both to get an idea of how well the model is performing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.656615Z",
     "iopub.status.busy": "2024-11-13T12:12:59.656459Z",
     "iopub.status.idle": "2024-11-13T12:12:59.670287Z",
     "shell.execute_reply": "2024-11-13T12:12:59.669828Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get the selected features after SFS for training and testing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_test_sfs \u001b[38;5;241m=\u001b[39m \u001b[43mX_test\u001b[49m[sfs_features]\n\u001b[1;32m      3\u001b[0m X_train_sfs \u001b[38;5;241m=\u001b[39m X_train[sfs_features]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train a neural network using SFS selected features\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the selected features after SFS for training and testing\n",
    "X_test_sfs = X_test[sfs_features]\n",
    "X_train_sfs = X_train[sfs_features]\n",
    "\n",
    "# Train a neural network using SFS selected features\n",
    "mlp_sfs = mlp.fit(X_train_sfs, y_train)\n",
    "y_pred_sfs = mlp_sfs.predict(X_test_sfs)\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) for SFS\n",
    "mse_sfs = mean_squared_error(y_test, y_pred_sfs)\n",
    "print(\"Mean Squared Error with SFS: \", mse_sfs)\n",
    "\n",
    "# Print the actual vs predicted values for SFS\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, y_pred_sfs, color='blue')\n",
    "plt.plot(y_test, y_test, color='red')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"SFS (Actual vs Predicted Values)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Network with SBS Features\n",
    "Similary we now also fit the model with the selected features of the sbs and calculate the mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.671998Z",
     "iopub.status.busy": "2024-11-13T12:12:59.671842Z",
     "iopub.status.idle": "2024-11-13T12:12:59.686300Z",
     "shell.execute_reply": "2024-11-13T12:12:59.685841Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get the selected features after SBS for training and testing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x_test_sbs \u001b[38;5;241m=\u001b[39m \u001b[43mX_test\u001b[49m[sbs_features]\n\u001b[1;32m      3\u001b[0m x_train_sbs \u001b[38;5;241m=\u001b[39m X_train[sbs_features]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train a neural network using SBS selected features\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the selected features after SBS for training and testing\n",
    "x_test_sbs = X_test[sbs_features]\n",
    "x_train_sbs = X_train[sbs_features]\n",
    "# Train a neural network using SBS selected features\n",
    "mlp_sbs = mlp.fit(x_train_sbs, y_train)\n",
    "y_pred_sbs = mlp_sbs.predict(x_test_sbs)\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) for SBS\n",
    "mse_sbs = mean_squared_error(y_test, y_pred_sbs)\n",
    "print(\"Mean Squared Error with SBS: \", mse_sbs)\n",
    "\n",
    "# Print the actual vs predicted values for SBS\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, y_pred_sbs, color='blue')\n",
    "plt.plot(y_test, y_test, color='red')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"SBS (Actual vs Predicted Values)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature projection\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique that can be used to reduce the number of features in a dataset by calculating the variance of each feature and selecting the most important ones. The higher the variance in a feature the more important it is to explain the dataset.\n",
    "\n",
    "Here we initialize a PCA object and fit it on the training data. We then plot the cumulative explained variance ratio to identify a reasonable number of components to select, where the curve starts to flatten out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.688026Z",
     "iopub.status.busy": "2024-11-13T12:12:59.687871Z",
     "iopub.status.idle": "2024-11-13T12:12:59.701545Z",
     "shell.execute_reply": "2024-11-13T12:12:59.700937Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLPRegressor(hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m50\u001b[39m,), max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_iter_no_change\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Fit the PCA object on the dataset\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pca\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Plot the explained variance ratio to be able to select the number of components\u001b[39;00m\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Import the PCA class\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize a default PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Keeping the mlp the same for a fair comparison\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(50,), max_iter=500, early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# Fit the PCA object on the dataset\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Plot the explained variance ratio to be able to select the number of components\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Cut-off Point\n",
    "From the plot, we can see a linear logarithmic graph, explaining to use that as the number of components increases, the explained variance increases at a decreasing rate. From the graph, we can identify that around the 22nd component, we can explain around 90% of the variance in the dataset. This is a normal threshold trade-off between the number of components and the explained variance.\n",
    "\n",
    "We then assign the selected components to a specific PCA object and fit the PCA object on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.703268Z",
     "iopub.status.busy": "2024-11-13T12:12:59.703112Z",
     "iopub.status.idle": "2024-11-13T12:12:59.705374Z",
     "shell.execute_reply": "2024-11-13T12:12:59.704930Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Select the threshold for component selection\n",
    "component_threshold = 22\n",
    "\n",
    "# Assign the selected components to a specific PCA object\n",
    "pca = PCA(n_components=component_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion on PCA\n",
    "\n",
    "**Strengths of PCA:**\n",
    "- PCA reduces the dimensionality of the dataset, which can lead to faster training times and reduced computational cost.\n",
    "- It can help to remove noise and redundant features, potentially improving model performance.\n",
    "- PCA can reveal the underlying structure of the data by identifying the principal components that explain the most variance.\n",
    "\n",
    "**Limitations of PCA:**\n",
    "- PCA is a linear method and may not capture complex, non-linear relationships in the data.\n",
    "- The principal components may not have a clear interpretation, making it difficult to understand the impact of individual features.\n",
    "- PCA requires the data to be standardized, which may not always be appropriate for all datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Network with PCA Features\n",
    "We can now transform and fit the training data, aswell as transform the test data. So that we can test the effectiveness of the PCA feature selection based on variance through a neural network.\n",
    "\n",
    "We then compute the Mean Squared Error (MSE) for PCA and also the extra plot the actual vs predicted values for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.707120Z",
     "iopub.status.busy": "2024-11-13T12:12:59.706958Z",
     "iopub.status.idle": "2024-11-13T12:12:59.720866Z",
     "shell.execute_reply": "2024-11-13T12:12:59.720410Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit and transform the training data \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pca_train \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Transform the test data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pca_test \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit and transform the training data \n",
    "pca_train = pca.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "pca_test = pca.transform(X_test)\n",
    "\n",
    "# Train a neural network using PCA selected features\n",
    "mlp_pca = mlp.fit(pca_train, y_train)\n",
    "\n",
    "# Predict on the test set after fitting the model with PCA\n",
    "pca_pred = mlp_pca.predict(pca_test)\n",
    "\n",
    "# Compute the Mean Squared Error (MSE) for PCA\n",
    "mse_pca = mean_squared_error(y_test, pca_pred)\n",
    "print(\"Mean Squared Error with PCA: \", mse_pca)\n",
    "\n",
    "# Plot the actual vs predicted values for PCA\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, pca_pred, color='blue')\n",
    "plt.plot(y_test, y_test, color='red')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"PCA (Actual vs Predicted Values)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Comparison\n",
    "\n",
    "The MSE values are printed near each other for comparison and a simple bar chart is plotted to visualize the differences. The method with the lowest bar indicates the lowest MSE value and therefore the best feature selection method for this dataset. From graph it is clear that the SBS method has the lowest MSE value. ollowed by the filler method then the PCA method and finally the SFS method. Note altough we are ordering the methods by their MSE values, the difference between the methods is very small and could be different each time the models are trained and predicted on. All methods perform very well and reach close to optimal predictions and MSE values.\n",
    "\n",
    "It was seen that both SBS and SFS reached the max number of features selected which was 40. This could be limiting the SFS method to select more features that could have improved the model. However this highlights one of its main flaws of being greedy and not considering the future information. \n",
    "\n",
    "The PCA method was able to reduce the dimensionality of the dataset while still maintaining the predictive power of the model. PCA shows to be a very good model if conserving computational power is a priority. \n",
    "\n",
    "The filter method using the corelation matrix showed to be more effective than expected, however, it makes sense as the corelation to the goal clearly identitfy the features that are more likely to be predictive.\n",
    "\n",
    "Finally, we conclude that for this dataset the SBS method was the best feature selection method. This could be due to the large number of features in the dataset and the ability of the SBS method beeing able to remove less important ones whilst keeping the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T12:12:59.722567Z",
     "iopub.status.busy": "2024-11-13T12:12:59.722411Z",
     "iopub.status.idle": "2024-11-13T12:12:59.735852Z",
     "shell.execute_reply": "2024-11-13T12:12:59.735393Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mse_flt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print the mse for all the methods\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Squared Error with Filter Method: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmse_flt\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Squared Error with SFS: \u001b[39m\u001b[38;5;124m\"\u001b[39m, mse_sfs)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Squared Error with SBS: \u001b[39m\u001b[38;5;124m\"\u001b[39m, mse_sbs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mse_flt' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the mse for all the methods\n",
    "print(\"Mean Squared Error with Filter Method: \", mse_flt)\n",
    "print(\"Mean Squared Error with SFS: \", mse_sfs)\n",
    "print(\"Mean Squared Error with SBS: \", mse_sbs)\n",
    "print(\"Mean Squared Error with PCA: \", mse_pca)\n",
    "\n",
    "# Plot the mse for comparison in a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Filter Method', 'SFS', 'SBS', 'PCA'], [mse_flt, mse_sfs, mse_sbs, mse_pca])\n",
    "plt.xlabel('Feature Selection Method')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Mean Squared Error by Feature Selection Method')\n",
    "# Zoom in due to the small differences\\\n",
    "plt.ylim(0.4,0.55)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plagiarism\n",
    "\n",
    "- Use of GitHub Copilot for syntax aiding, especially when it came to plotting graphs.\n",
    "- Use of the VS Code inbuilt function definition to understand the parameters of the functions used.\n",
    "- Use of pandas documentation to understand data manipulation, including handling missing data: [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cce3503-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
