{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Multi-label classification\n",
    "\n",
    "## Load Dataset\n",
    "Load the dataset from 'assignment-2/yeast.csv' using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004168</td>\n",
       "      <td>-0.170975</td>\n",
       "      <td>-0.156748</td>\n",
       "      <td>-0.142151</td>\n",
       "      <td>0.058781</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.197719</td>\n",
       "      <td>0.041850</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>-0.056617</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103956</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>-0.098986</td>\n",
       "      <td>-0.054501</td>\n",
       "      <td>-0.007970</td>\n",
       "      <td>0.049113</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>-0.077933</td>\n",
       "      <td>-0.080529</td>\n",
       "      <td>-0.016267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509949</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.293799</td>\n",
       "      <td>0.087714</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>-0.006411</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119092</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>0.072254</td>\n",
       "      <td>0.044512</td>\n",
       "      <td>-0.051467</td>\n",
       "      <td>0.074686</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>0.079438</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042037</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>-0.069483</td>\n",
       "      <td>0.081015</td>\n",
       "      <td>-0.048207</td>\n",
       "      <td>0.089446</td>\n",
       "      <td>-0.004947</td>\n",
       "      <td>0.064456</td>\n",
       "      <td>-0.133387</td>\n",
       "      <td>0.068878</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Att1      Att2      Att3      Att4      Att5      Att6      Att7  \\\n",
       "0  0.004168 -0.170975 -0.156748 -0.142151  0.058781  0.026851  0.197719   \n",
       "1 -0.103956  0.011879 -0.098986 -0.054501 -0.007970  0.049113 -0.030580   \n",
       "2  0.509949  0.401709  0.293799  0.087714  0.011686 -0.006411 -0.006255   \n",
       "3  0.119092  0.004412 -0.002262  0.072254  0.044512 -0.051467  0.074686   \n",
       "4  0.042037  0.007054 -0.069483  0.081015 -0.048207  0.089446 -0.004947   \n",
       "\n",
       "       Att8      Att9     Att10  ...  Class5  Class6  Class7  Class8  Class9  \\\n",
       "0  0.041850  0.066938 -0.056617  ...       0       0       1       1       0   \n",
       "1 -0.077933 -0.080529 -0.016267  ...       0       0       0       0       0   \n",
       "2  0.013646 -0.040666 -0.024447  ...       0       0       0       0       0   \n",
       "3 -0.007670  0.079438  0.062184  ...       0       0       0       0       0   \n",
       "4  0.064456 -0.133387  0.068878  ...       1       1       0       0       0   \n",
       "\n",
       "   Class10  Class11  Class12  Class13  Class14  \n",
       "0        0        0        1        1        0  \n",
       "1        0        0        0        0        0  \n",
       "2        0        0        1        1        0  \n",
       "3        0        0        0        0        0  \n",
       "4        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = 'yeast.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Print the head of the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset:  (2417, 117)\n",
      "Number of missing values in the dataset:  0\n",
      "Categorical columns in the dataset:  Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the dataset\n",
    "print(\"Shape of the dataset: \", data.shape)\n",
    "\n",
    "# Check how many missing values are in the dataset\n",
    "print(\"Number of missing values in the dataset: \",data.isnull().sum().sum())\n",
    "\n",
    "# Check for any categorical columns in the dataset\n",
    "print(\"Categorical columns in the dataset: \",data.select_dtypes(include=['object']).columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Preprocessing\n",
    "# Split the dataset into features and multi-label targets\n",
    "X = data.iloc[:, :103].values  # Features (first 103 columns)\n",
    "y = data.iloc[:, 103:].values  # Multi-label targets (last 14 columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Problem transformation \n",
    "\n",
    "### Binary Relavance vs Classifier Chains \n",
    "\n",
    "**Binary Relevance Approach**\n",
    "- *Independent Models*: Treats each label as an independent binary classification problem.\n",
    "- *Simplicity*: Simple to implement and understand.\n",
    "- *Scalability*: Scales well with a large number of labels since each label is treated separately.\n",
    "- *Label Dependency*: Ignores any potential dependencies or correlations between labels.\n",
    "- *Training*: Trains one binary classifier per label.\n",
    "\n",
    "**Classifier Chains Approach**\n",
    "- *Dependent Models*: Models the dependencies between labels by chaining classifiers.\n",
    "- *Complexity*: More complex to implement and understand compared to binary relevance.\n",
    "- *Scalability*: Can be computationally expensive with a large number of labels due to the chaining process.\n",
    "- *Label Dependency*: Takes into account the dependencies and correlations between labels, potentially leading to better performance.\n",
    "- *Training*: Trains classifiers sequentially, where each classifier considers the predictions of previous classifiers in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m models \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m---> 45\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralNetworkModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train[:, i], epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     47\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "Cell \u001b[0;32mIn[39], line 29\u001b[0m, in \u001b[0;36mNeuralNetworkModel\u001b[0;34m(input_dim)\u001b[0m\n\u001b[1;32m     26\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(input_dim,))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Add layers\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[1;32m     31\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n",
      "File \u001b[0;32m~/cce3503/cce3503-venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/cce3503/cce3503-venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:141\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(x, dtype, sparse)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mis_tensor(x):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_int_dtype(dtype):\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;66;03m# TensorFlow conversion is stricter than other backends, it does not\u001b[39;00m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;66;03m# allow ints for bools or floats for ints. We convert without dtype\u001b[39;00m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;66;03m# and cast instead.\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(x, dtype)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mInternalError\u001b[0m: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features with StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define a Neural Network Architecture model \n",
    "\n",
    "def NeuralNetworkModel(input_dim):\n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Add layers\n",
    "    x = Dense(64, activation='relu')(input_layer)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Train Binary Classifiers for Each Label\n",
    "models = []\n",
    "for i in range(y_train.shape[1]):\n",
    "    model = NeuralNetworkModel(X_train.shape[1])\n",
    "    model.fit(X_train, y_train[:, i], epochs=50, batch_size=32, verbose=0)\n",
    "    models.append(model)\n",
    "\n",
    "# Evaluate the Model\n",
    "y_pred = np.zeros(y_test.shape)\n",
    "for i, model in enumerate(models):\n",
    "    y_pred[:, i] = model.predict(X_test).flatten()\n",
    "\n",
    "# Convert predictions to binary\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy and F1 score for each label\n",
    "accuracies = [accuracy_score(y_test[:, i], y_pred[:, i]) for i in range(y_test.shape[1])]\n",
    "f1_scores = [f1_score(y_test[:, i], y_pred[:, i]) for i in range(y_test.shape[1])]\n",
    "\n",
    "print(\"Accuracies: \", accuracies)\n",
    "print(\"F1 Scores: \", f1_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cce3503-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
