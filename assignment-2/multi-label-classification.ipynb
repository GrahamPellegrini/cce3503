{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Multi-label classification\n",
    "\n",
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:50:21.566208Z",
     "iopub.status.busy": "2024-12-06T21:50:21.561646Z",
     "iopub.status.idle": "2024-12-06T21:50:22.158883Z",
     "shell.execute_reply": "2024-12-06T21:50:22.145785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\n\\n# Check if CUDA is available\\ncuda_available = torch.cuda.is_available()\\nprint(\"CUDA available: \", cuda_available)\\n\\n# Set device to CUDA if available, otherwise CPU\\ndevice = torch.device(\"cuda\" if cuda_available else \"cpu\")\\nprint(\"Device set to: \", device) \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA available: \", cuda_available)\n",
    "\n",
    "# Set device to CUDA if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
    "print(\"Device set to: \", device) \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Load the dataset from 'assignment-2/yeast.csv' using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:50:22.170658Z",
     "iopub.status.busy": "2024-12-06T21:50:22.170418Z",
     "iopub.status.idle": "2024-12-06T21:50:22.745927Z",
     "shell.execute_reply": "2024-12-06T21:50:22.745494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset:  (2417, 117)\n",
      "Number of missing values in the dataset:  0\n",
      "Categorical columns in the dataset:  Index([], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004168</td>\n",
       "      <td>-0.170975</td>\n",
       "      <td>-0.156748</td>\n",
       "      <td>-0.142151</td>\n",
       "      <td>0.058781</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.197719</td>\n",
       "      <td>0.041850</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>-0.056617</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103956</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>-0.098986</td>\n",
       "      <td>-0.054501</td>\n",
       "      <td>-0.007970</td>\n",
       "      <td>0.049113</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>-0.077933</td>\n",
       "      <td>-0.080529</td>\n",
       "      <td>-0.016267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509949</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.293799</td>\n",
       "      <td>0.087714</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>-0.006411</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119092</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>0.072254</td>\n",
       "      <td>0.044512</td>\n",
       "      <td>-0.051467</td>\n",
       "      <td>0.074686</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>0.079438</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042037</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>-0.069483</td>\n",
       "      <td>0.081015</td>\n",
       "      <td>-0.048207</td>\n",
       "      <td>0.089446</td>\n",
       "      <td>-0.004947</td>\n",
       "      <td>0.064456</td>\n",
       "      <td>-0.133387</td>\n",
       "      <td>0.068878</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Att1      Att2      Att3      Att4      Att5      Att6      Att7  \\\n",
       "0  0.004168 -0.170975 -0.156748 -0.142151  0.058781  0.026851  0.197719   \n",
       "1 -0.103956  0.011879 -0.098986 -0.054501 -0.007970  0.049113 -0.030580   \n",
       "2  0.509949  0.401709  0.293799  0.087714  0.011686 -0.006411 -0.006255   \n",
       "3  0.119092  0.004412 -0.002262  0.072254  0.044512 -0.051467  0.074686   \n",
       "4  0.042037  0.007054 -0.069483  0.081015 -0.048207  0.089446 -0.004947   \n",
       "\n",
       "       Att8      Att9     Att10  ...  Class5  Class6  Class7  Class8  Class9  \\\n",
       "0  0.041850  0.066938 -0.056617  ...       0       0       1       1       0   \n",
       "1 -0.077933 -0.080529 -0.016267  ...       0       0       0       0       0   \n",
       "2  0.013646 -0.040666 -0.024447  ...       0       0       0       0       0   \n",
       "3 -0.007670  0.079438  0.062184  ...       0       0       0       0       0   \n",
       "4  0.064456 -0.133387  0.068878  ...       1       1       0       0       0   \n",
       "\n",
       "   Class10  Class11  Class12  Class13  Class14  \n",
       "0        0        0        1        1        0  \n",
       "1        0        0        0        0        0  \n",
       "2        0        0        1        1        0  \n",
       "3        0        0        0        0        0  \n",
       "4        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = 'yeast.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(\"Shape of the dataset: \", data.shape)\n",
    "\n",
    "# Check how many missing values are in the dataset\n",
    "print(\"Number of missing values in the dataset: \",data.isnull().sum().sum())\n",
    "\n",
    "# Check for any categorical columns in the dataset\n",
    "print(\"Categorical columns in the dataset: \",data.select_dtypes(include=['object']).columns)\n",
    "\n",
    "# Print the head of the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:50:22.752906Z",
     "iopub.status.busy": "2024-12-06T21:50:22.752666Z",
     "iopub.status.idle": "2024-12-06T21:50:22.756022Z",
     "shell.execute_reply": "2024-12-06T21:50:22.755612Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "# Split the dataset into features and multi-label targets\n",
    "X = data.iloc[:, :103].values  # Features (first 103 columns)\n",
    "y = data.iloc[:, 103:].values  # Multi-label targets (last 14 columns)\n",
    "\n",
    "# Standardize the features before splitting the dataset\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Problem transformation \n",
    "\n",
    "### Binary Relavance vs Classifier Chains \n",
    "\n",
    "**Binary Relevance Approach**\n",
    "- *Independent Models*: Treats each label as an independent binary classification problem.\n",
    "- *Simplicity*: Simple to implement and understand.\n",
    "- *Scalability*: Scales well with a large number of labels since each label is treated separately.\n",
    "- *Label Dependency*: Ignores any potential dependencies or correlations between labels.\n",
    "- *Training*: Trains one binary classifier per label.\n",
    "\n",
    "**Classifier Chains Approach**\n",
    "- *Dependent Models*: Models the dependencies between labels by chaining classifiers.\n",
    "- *Complexity*: More complex to implement and understand compared to binary relevance.\n",
    "- *Scalability*: Can be computationally expensive with a large number of labels due to the chaining process.\n",
    "- *Label Dependency*: Takes into account the dependencies and correlations between labels, potentially leading to better performance.\n",
    "- *Training*: Trains classifiers sequentially, where each classifier considers the predictions of previous classifiers in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T21:50:22.757629Z",
     "iopub.status.busy": "2024-12-06T21:50:22.757463Z",
     "iopub.status.idle": "2024-12-06T21:50:24.309749Z",
     "shell.execute_reply": "2024-12-06T21:50:24.309148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Relevance Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.53      0.59       167\n",
      "           1       0.60      0.59      0.60       211\n",
      "           2       0.70      0.66      0.68       196\n",
      "           3       0.62      0.53      0.57       171\n",
      "           4       0.62      0.50      0.55       144\n",
      "           5       0.47      0.36      0.41       127\n",
      "           6       0.37      0.25      0.30        76\n",
      "           7       0.33      0.06      0.10        83\n",
      "           8       0.00      0.00      0.00        30\n",
      "           9       0.40      0.04      0.07        55\n",
      "          10       0.00      0.00      0.00        62\n",
      "          11       0.76      0.95      0.84       366\n",
      "          12       0.76      0.95      0.85       365\n",
      "          13       0.00      0.00      0.00         7\n",
      "\n",
      "   micro avg       0.68      0.62      0.65      2060\n",
      "   macro avg       0.45      0.39      0.40      2060\n",
      "weighted avg       0.61      0.62      0.60      2060\n",
      " samples avg       0.68      0.62      0.62      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the Neural Network base classifier\n",
    "# We add early stopping to prevent overfitting and reduce training time when the validation score stops improving and attempts to converge for a large number of iterations\n",
    "fixed_nn= MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42, early_stopping=True, n_iter_no_change=100)\n",
    "# At 10 n_iter_no_change, the classification report gave more labels with zero division error, so we increased it significantly to 100\n",
    "\n",
    "# Binary Relevance Classifier\n",
    "binary_relevance_clf = MultiOutputClassifier(fixed_nn)\n",
    "binary_relevance_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "y_pred_br = binary_relevance_clf.predict(X_test)\n",
    "print(\"Binary Relevance Classification Report: \")\n",
    "print(classification_report(y_test, y_pred_br, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Chains Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.53      0.59       167\n",
      "           1       0.60      0.52      0.56       211\n",
      "           2       0.71      0.65      0.68       196\n",
      "           3       0.65      0.60      0.63       171\n",
      "           4       0.63      0.51      0.56       144\n",
      "           5       0.52      0.35      0.42       127\n",
      "           6       0.38      0.32      0.34        76\n",
      "           7       0.35      0.29      0.32        83\n",
      "           8       0.50      0.07      0.12        30\n",
      "           9       0.35      0.13      0.19        55\n",
      "          10       0.30      0.11      0.16        62\n",
      "          11       0.77      0.92      0.84       366\n",
      "          12       0.77      0.92      0.84       365\n",
      "          13       0.00      0.00      0.00         7\n",
      "\n",
      "   micro avg       0.67      0.62      0.65      2060\n",
      "   macro avg       0.51      0.42      0.45      2060\n",
      "weighted avg       0.64      0.62      0.62      2060\n",
      " samples avg       0.67      0.63      0.62      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import ClassifierChain\n",
    "\n",
    "# Classifier Chains\n",
    "chain_clf = ClassifierChain(fixed_nn)\n",
    "\n",
    "# Train the model using Classifier Chains\n",
    "chain_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the Classifier Chains model\n",
    "y_pred_cc = chain_clf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Classifier Chains Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_cc, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Adapted algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/users/gpel0001/cce3503/cce3503-venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/users/gpel0001/cce3503/cce3503-venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/users/gpel0001/cce3503/cce3503-venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/users/gpel0001/cce3503/cce3503-venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/users/gpel0001/cce3503/cce3503-venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/users/gpel0001/cce3503/cce3503-venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/users/gpel0001/cce3503/cce3503-venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'estimator__activation': 'tanh', 'estimator__hidden_layer_sizes': (50,), 'estimator__learning_rate_init': 0.0001}\n",
      "Best Model - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.54      0.63       167\n",
      "           1       0.52      0.46      0.49       211\n",
      "           2       0.66      0.69      0.67       196\n",
      "           3       0.64      0.65      0.64       171\n",
      "           4       0.53      0.43      0.48       144\n",
      "           5       0.45      0.34      0.39       127\n",
      "           6       0.37      0.20      0.26        76\n",
      "           7       0.41      0.14      0.21        83\n",
      "           8       0.23      0.10      0.14        30\n",
      "           9       0.31      0.09      0.14        55\n",
      "          10       0.25      0.05      0.08        62\n",
      "          11       0.77      0.89      0.83       366\n",
      "          12       0.77      0.88      0.82       365\n",
      "          13       0.00      0.00      0.00         7\n",
      "\n",
      "   micro avg       0.66      0.60      0.62      2060\n",
      "   macro avg       0.48      0.39      0.41      2060\n",
      "weighted avg       0.62      0.60      0.59      2060\n",
      " samples avg       0.67      0.61      0.60      2060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Redefine the Neural Network base classifier making sure 'relu' is used as the activation function since it allows each label to be predicted independently\n",
    "neural_network = MLPClassifier(max_iter=1000, random_state=42, activation='relu', early_stopping=True, n_iter_no_change=100)\n",
    "\n",
    "# Through the use of the sklaern multioutput classifier, we can train a single model to predict multiple labels. Ensuring each lable is threated independently for cross-entropy loss calculations and wrapper methods can be used to train a single model to predict multiple labels.\n",
    "model = MultiOutputClassifier(fixed_nn)\n",
    "\n",
    "# Define a grid of hyperparameters to search over for hyperparameter optimization (HPO)\n",
    "param_grid = {\n",
    "    # The number of neurons in the hidden layer is important to vary as it can affect the model's capacity to learn and may differ for each label\n",
    "    'estimator__hidden_layer_sizes': [(50,), (100,), (150,)],\n",
    "    # Certian activation functions may be baised towards certain labels so it is important to vary this hyperparameter to give a balanced model\n",
    "    'estimator__activation': ['relu', 'tanh', 'logistic'],\n",
    "    # The learning rate is important feature in training a model and different labels may converge at different rates so it is important to vary this hyperparameter\n",
    "    'estimator__learning_rate_init': [1e-3, 1e-4, 1e-5]\n",
    "}\n",
    "\n",
    "# GridSearchCV from the sklearn library is used due to its systematic evaluation of hyperparameters and cross-validation. Especially useful for multi-label classification problems. \n",
    "# A KFold cross-validation with 5 splits is used to ensure the model is evaluated on different subsets of the data to ensure the model is generalizing well. So that is it trains on 4 folds and evaluates on the 5th fold.\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=make_scorer(f1_score, average='macro'), cv=5, n_jobs=-1)\n",
    "# n_jobs = -1 uses all available cores to speed up the grid search (parallel processing for CPUs)\n",
    "\n",
    "# Fit the defined model with the grid of hyperparameters to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from the grid search\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "\n",
    "# Extract the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict and evaluate from the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Best Model - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cce3503-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
